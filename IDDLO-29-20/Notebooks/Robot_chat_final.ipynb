{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dans ce Mini-projet, nous allons créer un Robot-chat relativement simple qui sera utilisé pour répondre aux questions fréquemment posées."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    #Installation des packages\n",
    "    Nous utiliserons simplement pip pour installer ce qui suit:\n",
    "    - numpy\n",
    "    - nltk\n",
    "    - tensorflow\n",
    "    - tflearn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Données d'entraînement\n",
    "Comme il s'agit d'un simple chat, on va créer un fichier .JSON qui contient les questions et les responses qu'on va utiliser pour le test. on va appeer notre fichier \"intents.json\" dans notre projet\n",
    "#Un extrait du fichier intent.json:\n",
    "{\"intents\": [\n",
    "        {\"tag\": \"greeting\",\n",
    "         \"patterns\": [\"Hi\", \"How are you\", \"Is anyone there?\", \"Hello\", \"Good day\", \"Whats up\"],\n",
    "         \"responses\": [\"Hello!\", \"Good to see you again!\", \"Hi there, how can I help?\"],\n",
    "         \"context_set\": \"\"\n",
    "        },\n",
    "Ce que nous faisons avec le fichier JSON, c'est créer un ensemble de messages que l'utilisateur est susceptible de saisir et de les mapper à un groupe de réponses appropriées. La balise de chaque dictionnaire du fichier indique le groupe auquel appartient également chaque message. Avec ces données, nous formerons un réseau de neurones pour prendre une phrase de mots et la classer comme l'une des balises de notre fichier. Ensuite, nous pouvons simplement prendre une réponse de ces groupes et l'afficher à l'utilisateur"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#1- Importation des bibliothèques Chargement de nos données JSON\n",
    "Nous allons commencer par importer quelques modules et charger dans nos données json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\helpers\\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow\n",
    "import random\n",
    "import json\n",
    "\n",
    "with open('intents.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    #2- Extraction de données\n",
    "    Maintenant on va parcourir nos données JSON et d'extraire les données que nous voulons. Pour chaque motif, nous le \n",
    "    transformerons en une liste de mots en utilisant \"nltk.word_tokenize\". Nous ajouterons ensuite chaque modèle dans notre \n",
    "    liste docs_words_tag et sa balise associée dans la liste docs_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    words = []\n",
    "    labels = []\n",
    "    docs_words_tok = []\n",
    "    docs_tag = []\n",
    "\n",
    "    for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            words.extend(wrds)\n",
    "            docs_words_tok.append(wrds)\n",
    "            docs_tag.append(intent[\"tag\"])\n",
    "\n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    #Racine de mots\n",
    "    Nous utiliserons ce processus de racine des mots (Stemmer) pour réduire le vocabulaire de notre modèle et tenter de \n",
    "    trouver le sens plus général derrière les phrases comme on vu dans le cours avec le professseur c'est l'un des algorithmes \n",
    "    du Natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "    words = sorted(list(set(words)))\n",
    "    labels = sorted(labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    #Bag of words\n",
    "    Comme nous le savons, les réseaux de neurones et les algorithmes d'apprentissage automatique nécessitent une entrée \n",
    "    numérique. Donc, nous avons besoin d'un moyen de représenter nos phrases avec des nombres et c'est là qu'un sac de \n",
    "    mots(bag of words) entre en jeu. Ce que nous allons faire est de représenter chaque phrase avec une liste de la longueur \n",
    "    de la quantité de mots dans le vocabulaire de nos modèles. Chaque position dans la liste représentera un mot de notre \n",
    "    vocabulaire. Si la position dans la liste est un 1, cela signifie que le mot existe dans notre phrase, s'il s'agit d'un 0, \n",
    "    alors le mot n'est pas présent.\n",
    "    En plus de formater notre entrée, nous devons formater notre sortie pour qu'elle ait un sens pour le réseau neuronal.Nous \n",
    "    créerons des listes de sortie qui correspondent à la longueur de la quantité des lables/tag que nous avons dans notre \n",
    "    ensemble de données. Chaque position dans la liste représentera un label/tag distincte, un 1 dans l'une de ces positions \n",
    "    indiquera un label/tag est représentée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "    for idx, doc in enumerate(docs_words_tok):\n",
    "        bag = []\n",
    "        \n",
    "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_tag[idx])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "   \n",
    "\n",
    "    training = numpy.array(training)\n",
    "    output = numpy.array(output)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    #3- Développer d'un modèle\n",
    "    Maintenant que nous avons prétraité toutes nos données, nous sommes prêts à commencer à créer et à entraîner un modèle. \n",
    "    Pour nos besoins, nous utiliserons un réseau de neurones à réaction assez standard avec deux couches cachées. Le but de \n",
    "    notre réseau sera de regarder un sac de mots et de donner une classe à laquelle ils appartiennent aussi (une de nos \n",
    "    balises du fichier JSON)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    #3-1 Nous commencerons par définir l'architecture de notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\layers\\core.py:81: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\layers\\core.py:145: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\initializations.py:174: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\optimizers.py:238: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\objectives.py:70: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\layers\\estimator.py:189: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\helpers\\trainer.py:571: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\helpers\\trainer.py:115: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\summaries.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\helpers\\trainer.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\helpers\\trainer.py:164: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\helpers\\trainer.py:165: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\helpers\\trainer.py:166: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my-pc\\Anaconda3\\envs\\nlplab\\lib\\site-packages\\tflearn\\helpers\\trainer.py:167: The name tf.get_collection_ref is deprecated. Please use tf.compat.v1.get_collection_ref instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensorflow.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "modele = tflearn.DNN(net)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#3-2 Entraînement et sauvegarde du modèle\n",
    "Maintenant nous adapterons nos données au modèle. Le nombre d'epoch que nous avons défini est le nombre de fois où le modèle verra les mêmes informations pendant l'entraînement.\n",
    "Une fois que nous avons terminé la creation du modèle, nous pouvons l'enregistrer dans le fichier modele.tflearn pour une utilisation dans d'autres scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3999  | total loss: \u001b[1m\u001b[32m0.58033\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1000 | loss: 0.58033 - acc: 0.9291 -- iter: 24/26\n",
      "Training Step: 4000  | total loss: \u001b[1m\u001b[32m0.53145\u001b[0m\u001b[0m | time: 0.020s\n",
      "| Adam | epoch: 1000 | loss: 0.53145 - acc: 0.9362 -- iter: 26/26\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\my-pc\\Desktop\\TAL-NLP-Project-master\\fileMourad\\modele.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "modele.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "modele.save(\"modele.tflearn\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    #4- Test du model(prédictions)\n",
    "          Notre processus de génération d'une réponse semblable à ce qui suit:\n",
    "        - Obtenez une entrée de l'utilisateur\n",
    "        - Convertissez-la en un sac de mots\n",
    "        - Obtenez une prédiction du modèle\n",
    "        - Trouvez la classe la plus probable\n",
    "        - Choisissez une réponse de cette classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La fonction bag_of_words transformera notre entrée de chaîne en un sac de mots en utilisant notre liste de mots créée\n",
    "\n",
    "def bag_of_words(sentence, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(sentence)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for sentc in s_words:\n",
    "        for indx, word in enumerate(words):\n",
    "            if word == sentc:\n",
    "                bag[indx] = 1\n",
    "            \n",
    "    return numpy.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencer la conversation avec le Robot (Tapez stop pour arrêter la concersation)!\n"
     ]
    }
   ],
   "source": [
    "#La fonction de chat gérera l'obtention d'une prédiction du modèle et la saisie d'une réponse \n",
    "# appropriée à partir de notre fichier de réponses JSON\n",
    "def chat():\n",
    "    print(\"Commencer la conversation avec le Robot (Tapez stop pour arrêter la concersation)!\")\n",
    "    while True:\n",
    "        entree = input(\"You: \")\n",
    "        if entree.lower() == \"stop\":\n",
    "            break\n",
    "\n",
    "        resultat = modele.predict([bag_of_words(entree, words)])\n",
    "        resultat_index = numpy.argmax(resultat)\n",
    "        tag = labels[resultat_index]\n",
    "\n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                reponses = tg['responses']\n",
    "\n",
    "        print(random.choice(reponses))\n",
    "#chat()\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
